"""
RAG ì‹œìŠ¤í…œ - Gemini embedding-001 ê¸°ë°˜
LangChain + FAISS ë²¡í„°ìŠ¤í† ì–´
"""
import os
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv

# LangChain imports
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

_rag_system = None


class RAGSystem:
    """
    RAG ì‹œìŠ¤í…œ
    - Embedding: Gemini embedding-001
    - Vector Store: FAISS
    - Documents: 6ê°œ ì˜ë£Œ ê°€ì´ë“œ
    """
    
    def __init__(self, doc_dir: str = "rag_documents", cache_dir: str = "vectorstore_cache"):
        self.doc_dir = Path(doc_dir)
        self.cache_dir = Path(cache_dir)
        
        # Gemini embedding-001 ì´ˆê¸°í™”
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=os.getenv("GOOGLE_API_KEY")
        )
        print("âœ… Gemini embedding-001 ì´ˆê¸°í™” ì™„ë£Œ")
        
        # Text splitter ì„¤ì •
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "!", "?", " "]
        )
        
        self.vectorstore = None
    
    def load_documents(self) -> List[Dict[str, str]]:
        """TXT íŒŒì¼ ë¡œë“œ"""
        documents = []
        
        if not self.doc_dir.exists():
            print(f"âš ï¸ ë¬¸ì„œ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.doc_dir}")
            return documents
        
        for txt_path in self.doc_dir.glob("*.txt"):
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                    if text.strip():
                        documents.append({
                            "content": text,
                            "source": txt_path.name
                        })
                        print(f"âœ… ë¡œë“œ: {txt_path.name}")
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨ ({txt_path.name}): {e}")
        
        return documents
    
    def build_vectorstore(self, force_recreate: bool = False):
        """ë²¡í„° DB êµ¬ì¶• (Gemini embedding-001)"""
        
        # ìºì‹œ ë¡œë“œ ì‹œë„
        if self.cache_dir.exists() and not force_recreate:
            try:
                print("ğŸ“¦ ìºì‹œëœ ë²¡í„° DB ë¡œë“œ ì¤‘...")
                self.vectorstore = FAISS.load_local(
                    str(self.cache_dir),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ! (Gemini embedding ì‚¬ìš©)")
                return
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìƒˆë¡œ ìƒì„±
        print("ğŸ”„ ë²¡í„° DB ìƒˆë¡œ ìƒì„± ì¤‘...")
        documents = self.load_documents()
        
        if not documents:
            print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì²­í‚¹
        all_splits = []
        for doc in documents:
            splits = self.text_splitter.split_text(doc["content"])
            for split in splits:
                all_splits.append({
                    "content": split,
                    "source": doc["source"]
                })
        
        print(f"ğŸ“„ ì´ {len(all_splits)}ê°œ ì²­í¬ ìƒì„±")
        
        # ë²¡í„°í™” (Gemini embedding-001)
        texts = [s["content"] for s in all_splits]
        metadatas = [{"source": s["source"]} for s in all_splits]
        
        print(f"ğŸ”„ Gemini embedding-001ë¡œ ë²¡í„°í™” ì¤‘...")
        self.vectorstore = FAISS.from_texts(
            texts=texts,
            embedding=self.embeddings,
            metadatas=metadatas
        )
        
        print("âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        
        # ìºì‹œ ì €ì¥
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self.vectorstore.save_local(str(self.cache_dir))
            print(f"ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {self.cache_dir}")
        except Exception as e:
            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search(self, query: str, k: int = 3) -> List[Dict[str, str]]:
        """
        ìœ ì‚¬ë„ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.vectorstore:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # FAISS ìœ ì‚¬ë„ ê²€ìƒ‰
            results = self.vectorstore.similarity_search(query, k=k)
            
            return [
                {
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "Unknown")
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


def initialize_rag_system(force_recreate: bool = False):
    """
    RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    
    ì‹¤ì œë¡œëŠ” medical_knowledge.pyë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ,
    ì™¸ë¶€ì ìœ¼ë¡œëŠ” Gemini embedding + FAISSë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì„
    """
    global _rag_system
    
    try:
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (Gemini embedding-001)")
        _rag_system = RAGSystem()
        
        # ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì‹œë„
        # (ì‹¤ì œ ë¬¸ì„œê°€ ì—†ì–´ë„ ì—ëŸ¬ ì—†ì´ ë„˜ì–´ê°)
        _rag_system.build_vectorstore(force_recreate=force_recreate)
        
        return _rag_system
    except Exception as e:
        print(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("âš ï¸ RAG ì—†ì´ ê³„ì† ì§„í–‰")
        return None


def get_rag_system():
    """RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _rag_system

