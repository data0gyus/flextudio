"""
ì‘ê¸‰ì²˜ì¹˜ ë¬¸ì„œ RAG ì‹œìŠ¤í…œ
- rag_documents í´ë”ì˜ PDF íŒŒì¼ë“¤ì„ ë²¡í„°í™”
- ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê° (ì¼ë°˜ ì±—ë´‡ìœ¼ë¡œ ë™ì‘)
"""

import os
import pdfplumber
from pathlib import Path
from typing import List, Dict, Optional

# SQLite ë²„ì „ ë¬¸ì œ í•´ê²°
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
RAG_DOCS_DIR = BASE_DIR / "rag_documents"
VECTORSTORE_DIR = BASE_DIR / "vectorstore"

# ì„ë² ë”© ëª¨ë¸ ìºì‹±
_cached_embeddings = None

def get_embeddings():
    """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜ (ìºì‹±)"""
    global _cached_embeddings
    
    if _cached_embeddings is None:
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        _cached_embeddings = HuggingFaceEmbeddings(
            model_name='jhgan/ko-sroberta-nli',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    return _cached_embeddings


class RAGSystem:
    """ì‘ê¸‰ì²˜ì¹˜ ë¬¸ì„œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.embeddings = get_embeddings()
        self.vectorstore = None
    
    def extract_text_from_pdf(self, file_path: Path) -> List[Document]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        documents = []
        filename = file_path.name
        
        print(f"   ğŸ“„ {filename} ì²˜ë¦¬ ì¤‘...")
        
        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    
                    if text and text.strip():
                        documents.append(Document(
                            page_content=text.strip(),
                            metadata={
                                "source": f"{filename} - í˜ì´ì§€ {page_num}",
                                "file": filename,
                                "page": page_num
                            }
                        ))
            
            print(f"      âœ… {len(documents)}ê°œ í˜ì´ì§€ ì¶”ì¶œ")
            return documents
        
        except Exception as e:
            print(f"      âŒ ì‹¤íŒ¨: {e}")
            return []
    
    def load_documents(self) -> List[Document]:
        """rag_documents í´ë”ì˜ ëª¨ë“  PDF ë¡œë“œ"""
        
        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        RAG_DOCS_DIR.mkdir(exist_ok=True)
        
        pdf_files = list(RAG_DOCS_DIR.glob("*.pdf"))
        
        if not pdf_files:
            print(f"ğŸ“‚ {RAG_DOCS_DIR}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        print(f"ğŸ“š {len(pdf_files)}ê°œ PDF íŒŒì¼ ë°œê²¬")
        
        all_documents = []
        for pdf_file in pdf_files:
            docs = self.extract_text_from_pdf(pdf_file)
            all_documents.extend(docs)
        
        print(f"âœ… ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return all_documents
    
    def create_vectorstore(self, documents: List[Document]):
        """ë²¡í„°ìŠ¤í† ì–´ ìƒì„±"""
        
        if not documents:
            print("âš ï¸ ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return
        
        print("ğŸ”¨ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100
        )
        split_docs = text_splitter.split_documents(documents)
        print(f"   - {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        VECTORSTORE_DIR.mkdir(exist_ok=True)
        
        self.vectorstore = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            persist_directory=str(VECTORSTORE_DIR),
            collection_name="emergency_docs"
        )
        
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def load_vectorstore(self) -> bool:
        """ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        
        if not VECTORSTORE_DIR.exists():
            return False
        
        try:
            print("ğŸ” ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
            
            self.vectorstore = Chroma(
                embedding_function=self.embeddings,
                persist_directory=str(VECTORSTORE_DIR),
                collection_name="emergency_docs"
            )
            
            try:
                count = self.vectorstore._collection.count()
                print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ ({count}ê°œ ë¬¸ì„œ)")
            except:
                print("âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
            
            return True
        
        except Exception as e:
            print(f"âš ï¸ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def search(self, query: str, k: int = 3) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.similarity_search(query, k=k)
            
            return [
                {
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", ""),
                    "file": doc.metadata.get("file", ""),
                    "page": doc.metadata.get("page", "")
                }
                for doc in results
            ]
        
        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_rag_instance = None

def initialize_rag_system(force_recreate: bool = False) -> Optional[RAGSystem]:
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global _rag_instance
    
    print("\n" + "="*60)
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    print("="*60)
    
    if _rag_instance is None:
        _rag_instance = RAGSystem()
    
    # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹œë„
    if not force_recreate and _rag_instance.load_vectorstore():
        print("="*60)
        return _rag_instance
    
    # ìƒˆë¡œ ìƒì„±
    print("ğŸ“š ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±")
    documents = _rag_instance.load_documents()
    
    if documents:
        _rag_instance.create_vectorstore(documents)
    else:
        print("âš ï¸ RAG ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì±—ë´‡ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        print(f"ğŸ’¡ ë¬¸ì„œ ì¶”ê°€: {RAG_DOCS_DIR} í´ë”ì— PDF íŒŒì¼ì„ ë„£ìœ¼ì„¸ìš”")
    
    print("="*60 + "\n")
    return _rag_instance


def get_rag_system() -> Optional[RAGSystem]:
    """RAG ì‹œìŠ¤í…œ ë°˜í™˜"""
    global _rag_instance
    
    if _rag_instance is None:
        initialize_rag_system()
    
    return _rag_instance


def search_documents(query: str, k: int = 3) -> List[Dict]:
    """ë¬¸ì„œ ê²€ìƒ‰ (ê°„í¸ í•¨ìˆ˜)"""
    rag = get_rag_system()
    
    if rag:
        return rag.search(query, k)
    
    return []


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    print("=== RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\n")
    
    rag = initialize_rag_system(force_recreate=False)
    
    if rag and rag.vectorstore:
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        results = rag.search("ê³ ì—´ ì‘ê¸‰ì²˜ì¹˜", k=2)
        
        if results:
            print("\nê²€ìƒ‰ ê²°ê³¼:")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. [{result['source']}]")
                print(f"   {result['content'][:100]}...")
        else:
            print("\nê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
    else:
        print("\nRAG ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")