import os
from pathlib import Path
from typing import List, Dict
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

_rag_system = None

class RAGSystem:
    def __init__(self, doc_dir: str = "rag_documents", cache_dir: str = "vectorstore_cache"):
        self.doc_dir = Path(doc_dir)
        self.cache_dir = Path(cache_dir)
        
        print("ğŸ”§ Gemini Embeddings API ì´ˆê¸°í™”...")
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=GOOGLE_API_KEY
        )
        
        # ì²­í¬ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=30,
            separators=["\n\n", "\n", ". ", " "]
        )
        
        self.vectorstore = None
    
    def load_documents(self) -> List[Dict[str, str]]:
        """TXT íŒŒì¼ ë¡œë“œ"""
        documents = []
        
        if not self.doc_dir.exists():
            print(f"âš ï¸ ë¬¸ì„œ ë””ë ‰í† ë¦¬ ì—†ìŒ: {self.doc_dir}")
            return documents
        
        txt_files = list(self.doc_dir.glob("*.txt"))
        
        if not txt_files:
            print(f"âš ï¸ TXT íŒŒì¼ ì—†ìŒ: {self.doc_dir}")
            return documents
        
        for txt_path in txt_files:
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                    
                    if text.strip():
                        documents.append({
                            "content": text,
                            "source": txt_path.name
                        })
                        print(f"âœ… ë¡œë“œ: {txt_path.name}")
            
            except Exception as e:
                print(f"âŒ TXT ë¡œë“œ ì‹¤íŒ¨ ({txt_path.name}): {e}")
        
        return documents
    
    def build_vectorstore(self, force_recreate: bool = False):
        """ë²¡í„° DB ìƒì„± (ìºì‹± í¬í•¨)"""
        
        # ğŸ”¥ ìºì‹œ ì¡´ì¬ í™•ì¸
        if self.cache_dir.exists() and not force_recreate:
            try:
                print("ğŸ“¦ ìºì‹œëœ ë²¡í„° DB ë¡œë“œ ì¤‘...")
                self.vectorstore = FAISS.load_local(
                    str(self.cache_dir),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ! (API í˜¸ì¶œ 0íšŒ)")
                return
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ğŸ”„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ğŸ“„ ë¬¸ì„œ ë¡œë“œ
        documents = self.load_documents()
        
        if not documents:
            print("âš ï¸ ë¡œë“œí•  ë¬¸ì„œ ì—†ìŒ")
            return
        
        # âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í• 
        all_splits = []
        for doc in documents:
            splits = self.text_splitter.split_text(doc["content"])
            for split in splits:
                all_splits.append({
                    "content": split,
                    "source": doc["source"]
                })
        
        print(f"ğŸ“„ ì´ {len(all_splits)}ê°œ ì²­í¬ ìƒì„±")
        
        # ì²­í¬ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        max_chunks = 500
        if len(all_splits) > max_chunks:
            print(f"âš ï¸ ì²­í¬ ìˆ˜ ì œí•œ: {len(all_splits)} â†’ {max_chunks}")
            all_splits = all_splits[:max_chunks]
        
        # ğŸŒ ë²¡í„° DB ìƒì„± (API í˜¸ì¶œ ë°œìƒ)
        texts = [s["content"] for s in all_splits]
        metadatas = [{"source": s["source"]} for s in all_splits]
        
        print(f"ğŸ”„ ë²¡í„° DB ìƒì„± ì¤‘... ({len(texts)}ê°œ ì„ë² ë”© API í˜¸ì¶œ)")
        self.vectorstore = FAISS.from_texts(
            texts=texts,
            embedding=self.embeddings,
            metadatas=metadatas
        )
        
        print(f"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ!")
        
        # ğŸ’¾ ë””ìŠ¤í¬ì— ì €ì¥
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self.vectorstore.save_local(str(self.cache_dir))
            print(f"ğŸ’¾ ë²¡í„° DB ìºì‹œ ì €ì¥ ì™„ë£Œ: {self.cache_dir}")
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search(self, query: str, k: int = 3) -> List[Dict[str, str]]:
        """ê²€ìƒ‰"""
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.similarity_search(query, k=k)
            return [
                {
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "Unknown")
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


def initialize_rag_system(force_recreate: bool = False):
    """RAG ì´ˆê¸°í™”"""
    global _rag_system
    
    try:
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ìºì‹± í¬í•¨)")
        _rag_system = RAGSystem()
        _rag_system.build_vectorstore(force_recreate=force_recreate)
        return _rag_system
    except Exception as e:
        print(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("âš ï¸ RAG ì—†ì´ ê³„ì† ì§„í–‰")
        return None

def get_rag_system():
    return _rag_system