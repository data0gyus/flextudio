"""
RAG ì‹œìŠ¤í…œ - í•œêµ­ì–´ ìµœì í™” (Render 512MB)
"""
import os
from pathlib import Path
from typing import List, Dict
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

load_dotenv()

_rag_system = None

class RAGSystem:
    def __init__(self, doc_dir: str = "rag_documents", cache_dir: str = "vectorstore_cache"):
        self.doc_dir = Path(doc_dir)
        self.cache_dir = Path(cache_dir)
        
        print("ğŸ¤— í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”...")
        print("   ëª¨ë¸: jhgan/ko-sroberta-multitask")
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",  # â† í•œêµ­ì–´ ìµœì !
            model_kwargs={'device': 'cpu'},
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 8,  # ë©”ëª¨ë¦¬ ì ˆì•½
                'show_progress_bar': False
            },
            cache_folder="/tmp/hf_cache"
        )
        print("âœ… í•œêµ­ì–´ ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "!", "?", " "]
        )
        
        self.vectorstore = None
    
    def load_documents(self) -> List[Dict[str, str]]:
        """TXT íŒŒì¼ ë¡œë“œ"""
        documents = []
        
        if not self.doc_dir.exists():
            return documents
        
        for txt_path in self.doc_dir.glob("*.txt"):
            try:
                with open(txt_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                    if text.strip():
                        documents.append({
                            "content": text,
                            "source": txt_path.name
                        })
                        print(f"âœ… ë¡œë“œ: {txt_path.name}")
            except Exception as e:
                print(f"âŒ ì‹¤íŒ¨ ({txt_path.name}): {e}")
        
        return documents
    
    def build_vectorstore(self, force_recreate: bool = False):
        """ë²¡í„° DB ë¡œë“œ (ìºì‹œ ìš°ì„ )"""
        
        # ìºì‹œ ë¡œë“œ
        if self.cache_dir.exists() and not force_recreate:
            try:
                print("ğŸ“¦ ìºì‹œëœ ë²¡í„° DB ë¡œë“œ ì¤‘...")
                self.vectorstore = FAISS.load_local(
                    str(self.cache_dir),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("âœ… ìºì‹œ ë¡œë“œ ì™„ë£Œ! (ì„ë² ë”© 0íšŒ)")
                return
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ìºì‹œ ì—†ìœ¼ë©´ ê²½ê³ 
        print("âš ï¸ ë²¡í„° DB ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë¡œì»¬ì—ì„œ create_cache_local.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        if os.getenv("RENDER"):
            print("ğŸš¨ Renderì—ì„œëŠ” ìºì‹œ í•„ìˆ˜ì…ë‹ˆë‹¤!")
            return
        
        # ë¡œì»¬ì—ì„œë§Œ ìƒì„±
        self._build_from_scratch()
    
    def _build_from_scratch(self):
        """ìƒˆë¡œ ìƒì„± (ë¡œì»¬ ì „ìš©)"""
        documents = self.load_documents()
        if not documents:
            return
        
        all_splits = []
        for doc in documents:
            splits = self.text_splitter.split_text(doc["content"])
            for split in splits:
                all_splits.append({
                    "content": split,
                    "source": doc["source"]
                })
        
        print(f"ğŸ“„ ì´ {len(all_splits)}ê°œ ì²­í¬")
        
        max_chunks = 400
        if len(all_splits) > max_chunks:
            all_splits = all_splits[:max_chunks]
        
        texts = [s["content"] for s in all_splits]
        metadatas = [{"source": s["source"]} for s in all_splits]
        
        print(f"ğŸ”„ ë²¡í„° DB ìƒì„± ì¤‘...")
        self.vectorstore = FAISS.from_texts(
            texts=texts,
            embedding=self.embeddings,
            metadatas=metadatas
        )
        
        print("âœ… ìƒì„± ì™„ë£Œ!")
        
        # ì €ì¥
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self.vectorstore.save_local(str(self.cache_dir))
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {self.cache_dir}")
        except Exception as e:
            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search(self, query: str, k: int = 3) -> List[Dict[str, str]]:
        """ê²€ìƒ‰"""
        if not self.vectorstore:
            return []
        
        try:
            results = self.vectorstore.similarity_search(query, k=k)
            return [
                {
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "Unknown")
                }
                for doc in results
            ]
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


def initialize_rag_system(force_recreate: bool = False):
    """RAG ì´ˆê¸°í™”"""
    global _rag_system
    
    try:
        print("ğŸš€ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í•œêµ­ì–´ ìµœì í™”)")
        _rag_system = RAGSystem()
        _rag_system.build_vectorstore(force_recreate=force_recreate)
        return _rag_system
    except Exception as e:
        print(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        print("âš ï¸ RAG ì—†ì´ ê³„ì† ì§„í–‰")
        return None

def get_rag_system():
    return _rag_system